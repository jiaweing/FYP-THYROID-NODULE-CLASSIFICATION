# -*- coding: utf-8 -*-
"""ResNet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GWi4xBtz7tt4-mGEpRs75m98kmNSNdOu
"""

from google.colab import drive
drive.mount('/content/drive')

#
# **************************************************************************************************************************************************
#	Libraries
# **************************************************************************************************************************************************
#

from keras import optimizers
from keras import applications
from keras.models import Model
from keras.layers import Flatten, Dense
from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping, ModelCheckpoint
from math import ceil

import numpy as np
import pandas as pd
import datetime
import json
import matplotlib.pyplot as plt

#
# **************************************************************************************************************************************************
#	Global Variables
# **************************************************************************************************************************************************
#

# dimensions of architecture
img_width, img_height = 224, 224

# saved filename of model
model_filename = '/content/drive/My Drive/Colab Notebooks/model.h5'

# saved filename of model weights
weights_filename = '/content/drive/My Drive/Colab Notebooks/weights.h5'
modelcheckpoint_filename = '/content/drive/My Drive/Colab Notebooks/model_checkpoint.h5'

# savae filename of results csv
results_filename = '/content/drive/My Drive/Colab Notebooks/results.csv'

# datasets' directories
train_data_dir = '/content/drive/My Drive/Colab Notebooks/train/'
# validation_data_dir = '/content/drive/My Drive/Colab Notebooks/validation/'
test_data_dir = '/content/drive/My Drive/Colab Notebooks/test/'

# number of channels for the architecture
channels = 3

# number of epochs
epochs = 500

# batch size 
train_batch_size = 2
validation_batch_size = 2

# learning rate
learning_rate = 0.0001

# class mode
class_mode = 'categorical'

# total number of classes
total_classes = 2

#
# **************************************************************************************************************************************************
#	Functions
# **************************************************************************************************************************************************
#
def summary():
	# model summary
	resnet50.summary()

	# graphing our training and validation
	acc = history.history['acc']
	val_acc = history.history['val_acc']
	loss = history.history['loss']
	val_loss = history.history['val_loss']
	epochs = range(len(acc))
	plt.plot(epochs, acc, 'r', label='Training Acc')
	plt.plot(epochs, val_acc, 'b', label='Validation Acc')
	plt.title('Training and Validation Accuracy')
	plt.ylabel('Accuracy')
	plt.xlabel('Epoch')
	plt.legend()
	plt.figure()
	plt.plot(epochs, loss, 'r', label='Training Loss')
	plt.plot(epochs, val_loss, 'b', label='Validation Loss')
	plt.title('Training and Validation Loss')
	plt.ylabel('loss')
	plt.xlabel('Epoch')
	plt.legend()
	plt.show()
 
def saveModel():
  model.save(model_filename)
  model.save_weights(weights_filename)

  model_json = model.to_json()
  with open("model_in_json.json","w") as json_file:
    json.dump(model_json, json_file)


#
# **************************************************************************************************************************************************
#	Main Code
# **************************************************************************************************************************************************
#

# loading resnet50 model
model = applications.ResNet50(include_top = False, weights = 'imagenet', classes = total_classes, input_shape=(img_width, img_height, channels))
flat = Flatten()(model.layers[-1].output)
output = Dense(total_classes, activation='softmax')(flat)
resnet50 = Model(input=model.inputs,outputs = output)

train_datagen = ImageDataGenerator(rescale = 1. / 255,
                                    rotation_range = 90,
                                    brightness_range = [0.2, 1.0],
                                    shear_range = 0.2,
                                    zoom_range = 0.3,
                                    width_shift_range = 1.0,
                                    height_shift_range = 1.0,
                                    horizontal_flip = True,
                                    vertical_flip = True,
                                    validation_split = 0.4)

validation_datagen = ImageDataGenerator(rescale=1. / 255)

# current time
start = datetime.datetime.now()

training_set = train_datagen.flow_from_directory(
                                		train_data_dir,
                                		target_size = (img_width, img_height),
                                		batch_size = train_batch_size,
                                		class_mode=class_mode,
																		subset = "training",
																		seed = 42,
                                		shuffle = True)

nb_train_samples = len(training_set.filenames)

# validation data
validation_set = train_datagen.flow_from_directory(
                                  		train_data_dir,
                                  		target_size=(img_width, img_height),
                                  		batch_size=validation_batch_size,
                                  		class_mode=class_mode,
																			subset = "validation",
																			seed = 42,
                                  		shuffle=True)

nb_validation_samples = len(validation_set.filenames)

resnet50.compile(loss = 'categorical_crossentropy',
                 optimizer = optimizers.Adam(lr = learning_rate),
                 metrics = ['acc'])

es = EarlyStopping(monitor = 'val_loss',
                    min_delta = 1e-5,
                    patience = 5,
                    verbose = 1,
                    mode = 'min',
                    restore_best_weights = True)

mc = ModelCheckpoint(modelcheckpoint_filename,
                        monitor = 'val_loss',
                        mode = 'min',
                        save_weights_only = True,
                        save_best_only = True)

history = resnet50.fit_generator(training_set,
                         steps_per_epoch = nb_train_samples / train_batch_size,
                         epochs = epochs,
												 callbacks = [es,mc],
                         validation_data = validation_set,
                         validation_steps = nb_validation_samples / validation_batch_size)

summary()
saveModel()

(eval_loss, eval_accuracy) = resnet50.evaluate_generator(
 validation_set, steps = nb_validation_samples, verbose = 1)

print("[INFO] Accuracy: {:.2f}%".format(eval_accuracy * 100))
print("[INFO] Loss: {}".format(eval_loss))
end = datetime.datetime.now()
elapsed = end - start
print('[INFO] Time Elapsed: ', elapsed)

# # test data
# test_datagen = ImageDataGenerator()
# test_set = test_datagen.flow_from_directory(
#                                   		test_data_dir,
#                                   		target_size = (img_width, img_height),
#                                   		batch_size = 1,
#                                   		class_mode = class_mode,
# 																			seed = 42,
#                                   		shuffle = True)

# step_size_test = test_set.n // test_set.batch_size
# test_set.reset()

# pred = resnet50.predict_generator(test_set,
#                                 steps = step_size_test,
#                                 verbose = 1)

# predicted_class_indices = np.argmax(pred, axis = 1)
# labels = (test_set.class_indices)

# labels = dict((v, k) for k, v in labels.items())
# predictions = [labels[k] for k in predicted_class_indices]

# filenames = test_set.filenames
# results = pd.DataFrame({"Filename": filenames,
#                       "Prediction": predictions})
# results.to_csv(results_filename, index = False)